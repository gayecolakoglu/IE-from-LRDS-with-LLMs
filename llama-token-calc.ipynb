{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7549ec-9ea4-4173-9750-fbd33cf34749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaye/miniconda3/envs/vt2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gaye/miniconda3/envs/vt2/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from os import path\n",
    "import itertools\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from modules.data_processing import categorize_files_by_template, extract_text_from_pdf\n",
    "from modules.chunking import get_chunk_size, generate_chunk\n",
    "from modules.arrange_conf import get_log_directory, get_output_directory, get_data_paths\n",
    "from modules.model_interaction import process_chunks_token_counts\n",
    "from config import hf_token, MODEL_llama, few_shot_examples\n",
    "from multiprocessing import current_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05425e86-84ba-4123-a061-cc119c792f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:10:48,274 - INFO - Logging setup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_directory:  llama3_70b_outputs/reg/\n",
      "base_output_directory:  llama3_70b_outputs/reg/\n",
      "folder_path: VT2/vrdu2/registration-form/main/pdfs\n",
      "dataset_path: VT2/vrdu2/registration-form/main/dataset.jsonl.gz\n",
      "dtype: reg\n"
     ]
    }
   ],
   "source": [
    "directory = 'vrdu2/registration-form/few_shot-splits/'\n",
    "# directory = 'vrdu2/ad-buy-form/few_shot-splits/'\n",
    "\n",
    "# Arrange logging and output dirs accordÄ±ng to model and form type\n",
    "model = MODEL_llama # MODEL_gpt_3 #MODEL_gpt_4 #  or \n",
    "log_directory = get_log_directory(directory, model) \n",
    "base_output_directory = get_output_directory(directory, model)\n",
    "\n",
    "print(\"log_directory: \", log_directory)\n",
    "print(\"base_output_directory: \", base_output_directory)\n",
    "\n",
    "# Arrange folder and dataset dirs based on the provided main dir\n",
    "folder_path, dataset_path, dtype = get_data_paths(directory)\n",
    "\n",
    "print(\"folder_path:\", folder_path)\n",
    "print(\"dataset_path:\", dataset_path)\n",
    "print(\"dtype:\", dtype)\n",
    "\n",
    "log_file_path = path.join(log_directory, f\"experiment_{current_process().pid}.log\")\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(log_file_path),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Example logging\n",
    "logging.info(\"Logging setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df89ca1-d813-49a0-bcdf-71cb43cde45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer with authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-70B\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df29ceb8-dcf0-4867-9ee6-2af629e4eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment_token_count(sample_size, use_custom_ocr, experiment_id, prompt_type, chunk_size_category, **kwargs):\n",
    "    logging.info(f\"Starting experiment with ID {experiment_id}\")\n",
    "    try:\n",
    "        logging.debug(\"Experiment parameters: \"\n",
    "                      f\"sample_size={sample_size}, use_custom_ocr={use_custom_ocr}, \"\n",
    "                      f\"prompt_type={prompt_type}, chunk_size_category={chunk_size_category}, kwargs={kwargs}\")\n",
    "\n",
    "        # Initialize variables and perform checks\n",
    "        example_num = kwargs.get('example_num', None)\n",
    "        no_schema = kwargs.get('no_schema', True)\n",
    "        transformation_method = kwargs.get('transformation_method', 'naive')\n",
    "        chunking_method = kwargs.get('chunking_method', 'fixed')\n",
    "        overlap = kwargs.get('overlap', None)\n",
    "        level_type = kwargs.get('level_type', 'STL')\n",
    "\n",
    "        template_types = ['Amendment', 'Dissemination', 'Short-Form']\n",
    "        logging.debug(f\"Categorizing files by template for experiment ID {experiment_id}\")\n",
    "\n",
    "        categorized_files = categorize_files_by_template(os.path.join(directory, 'prompts'))\n",
    "        logging.debug(f\"Loaded categorized files for level_type {level_type}\")\n",
    "\n",
    "        output_data = []\n",
    "        ocr_type = \"custom\" if use_custom_ocr else \"vrdu\"\n",
    "\n",
    "        for template_type in template_types:\n",
    "            print(\"s---------------Templete type: \", template_type)\n",
    "            print(\"s---------------Level type: \", level_type) \n",
    "            logging.info(f\"Processing template type: {template_type}\")\n",
    "            files_list = list(categorized_files[level_type][template_type])\n",
    "            sample_files = files_list[:sample_size]\n",
    "            logging.debug(f\"Loaded sample files for template type {template_type}: {sample_files}\")\n",
    "\n",
    "            examples = few_shot_examples[level_type][template_type][example_num] if not no_schema and example_num else None\n",
    "\n",
    "            for filename in sample_files:\n",
    "                print(\"s---------------Fiename: \", filename) \n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    start_time = time.time()\n",
    "                    logging.info(f\"Experiment {experiment_id}: Processing file {filename}\")\n",
    "\n",
    "                    success = False\n",
    "                    retry_count = 0\n",
    "                    max_retries = 10\n",
    "\n",
    "                    while not success and retry_count < max_retries:\n",
    "                        try:\n",
    "                            pdf_path = os.path.join(folder_path, filename)\n",
    "                            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                                temp_pdf_path = os.path.join(temp_dir, filename)\n",
    "                                shutil.copy(pdf_path, temp_pdf_path)\n",
    "\n",
    "                                logging.debug(f\"Extracting text from PDF for file {filename}\")\n",
    "                                extracted_data = extract_text_from_pdf(temp_pdf_path, dataset_path, use_custom_ocr, transformation_method=transformation_method)\n",
    "                                chunk_size = get_chunk_size(chunk_size_category, prompt_type, example_num)\n",
    "                                logging.debug(f\"Generating chunks for file {filename}\")\n",
    "                                chunks, prompt_token_size = generate_chunk(chunking_method, extracted_data, chunk_size, overlap)\n",
    "\n",
    "                                if not chunks:\n",
    "                                    raise ValueError(f\"Chunk generation failed for file {filename}\")\n",
    "                                    break\n",
    "\n",
    "                                logging.debug(f\"Processing chunks for file {filename}\")\n",
    "                                token_counts = process_chunks_token_counts(\n",
    "                                    chunks, prompt_type, examples, template_type, level_type,\n",
    "                                    MODEL_llama, tokenizer, dtype,\n",
    "                                    example_num\n",
    "                                )\n",
    "                                \n",
    "                                logging.debug(f\"Total count for file: {filename} is {token_counts}\")\n",
    "                                output_data.append({\n",
    "                                    \"experiment_id\": experiment_id,\n",
    "                                    \"model_name\": MODEL_llama,\n",
    "                                    \"sample_num\": len(sample_files),\n",
    "                                    \"chunk_size\": chunk_size,\n",
    "                                    \"prompt_type\": prompt_type,\n",
    "                                    \"example_num\": example_num,\n",
    "                                    \"level_type\": level_type,\n",
    "                                    \"file_name\": filename,\n",
    "                                    \"token_counts\": token_counts\n",
    "                                })\n",
    "                            success = True\n",
    "                        except requests.exceptions.HTTPError as e:\n",
    "                            if e.response.status_code == 429:\n",
    "                                retry_count += 1\n",
    "                                error_details = e.response.json()\n",
    "                                retry_after = parse_retry_after(error_details)\n",
    "\n",
    "                                if retry_after:\n",
    "                                    logging.warning(f\"Rate limit reached. Retrying in {retry_after:.2f} seconds due to rate limit.\")\n",
    "                                    print(f\"Rate limit reached. Retrying in {retry_after:.2f} seconds due to rate limit.\")\n",
    "                                    time.sleep(retry_after)\n",
    "                                    logging.info(f\"Waited for {retry_after:.2f} seconds before retrying.\")\n",
    "                                    print(f\"Waited for {retry_after:.2f} seconds before retrying.\")\n",
    "                                else:\n",
    "                                    delay = adaptive_delay(retry_count)\n",
    "                                    logging.warning(f\"No specific retry_after provided. Using adaptive delay of {delay:.2f} seconds.\")\n",
    "                                    print(f\"No specific retry_after provided. Using adaptive delay of {delay:.2f} seconds.\")\n",
    "                                    time.sleep(delay)\n",
    "                            else:\n",
    "                                logging.error(f\"HTTP error while processing file {filename}: {e}\")\n",
    "                                print(f\"HTTP error while processing file {filename}: {e}\")\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"An error occurred while processing file {filename}: {e}\")\n",
    "                            print(f\"An error occurred while processing file {filename}: {e}\")\n",
    "                            break\n",
    "\n",
    "        experiment_file_path = os.path.join(base_output_directory, 'token_counts')\n",
    "        output_file = os.path.join(experiment_file_path, f\"experiment_{experiment_id}_results.csv\")\n",
    "        result_df = pd.DataFrame(output_data)\n",
    "        result_df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Experiment {experiment_id}: Results written to {output_file}\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during experiment {experiment_id}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbdbdbe-801c-45dd-bba8-6e5aae49418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to llama3_70b_outputs/reg/all_experiment_combinations.csv\n"
     ]
    }
   ],
   "source": [
    "use_custom_ocr = False\n",
    "sample_size = 40\n",
    "prompt_types = [\"no_schema\", \"few_shot\", \"chain_of_thought\"]\n",
    "example_num_options = [0, 1, 3, 5]\n",
    "transformation_methods = [\"layout-aware\", \"naive\"]\n",
    "chunking_method = 'fixed'\n",
    "chunk_size_categories = [\"max\", \"medium\", \"small\"]\n",
    "level_types = [\"STL\", \"UTL\"]\n",
    "\n",
    "combinations = []\n",
    "experiment_id = 0\n",
    "\n",
    "for prompt_type, chunk_size_category, level_type in itertools.product(prompt_types, chunk_size_categories, level_types):\n",
    "    if prompt_type == \"no_schema\":\n",
    "        for transformation_method in transformation_methods:\n",
    "            combination = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'sample_size': sample_size,\n",
    "                'use_custom_ocr': use_custom_ocr,\n",
    "                'prompt_type': prompt_type,\n",
    "                'chunk_size_category': chunk_size_category,\n",
    "                'no_schema': True,\n",
    "                'example_num': None,\n",
    "                'transformation_method': transformation_method,\n",
    "                'chunking_method': chunking_method,\n",
    "                'level_type': level_type\n",
    "            }\n",
    "            combinations.append(combination)\n",
    "            experiment_id += 1\n",
    "    else:\n",
    "        for example_num, transformation_method in itertools.product(example_num_options, transformation_methods):\n",
    "            combination = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'sample_size': sample_size,\n",
    "                'use_custom_ocr': use_custom_ocr,\n",
    "                'prompt_type': prompt_type,\n",
    "                'chunk_size_category': chunk_size_category,\n",
    "                'no_schema': False,\n",
    "                'example_num': example_num,\n",
    "                'transformation_method': transformation_method,\n",
    "                'chunking_method': chunking_method,\n",
    "                'level_type': level_type\n",
    "            }\n",
    "            combinations.append(combination)\n",
    "            experiment_id += 1\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(combinations)\n",
    "\n",
    "# Saving to CSV\n",
    "csv_file_path = os.path.join(base_output_directory, 'all_experiment_combinations.csv')\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd5ea6-d825-4605-846a-02e3186bf3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:10:48,853 - INFO - Starting experiment with ID 0\n",
      "2025-02-19 11:10:48,857 - INFO - Processing template type: Amendment\n",
      "2025-02-19 11:10:48,858 - INFO - Experiment 0: Processing file 20030714_Arnold _ Porter Kaye Scholer, LLP_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Templete type:  Amendment\n",
      "s---------------Level type:  STL\n",
      "s---------------Fiename:  20030714_Arnold _ Porter Kaye Scholer, LLP_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:11:02,120 - INFO - Text length: 4437, Chunk size: 2958\n",
      "2025-02-19 11:11:02,122 - INFO - Generated chunk: OMB NO. 1105-0004 U.. Department of Justice Washin... with length 4436\n",
      "2025-02-19 11:11:02,123 - INFO - Generated 1 chunks with total text length 4436\n",
      "2025-02-19 11:11:02,124 - INFO - Generated 1 chunks with total prompt token size 694\n",
      "2025-02-19 11:11:02,134 - INFO - Chunk 1/1: Token Count = 1003\n",
      "2025-02-19 11:11:02,136 - INFO - Experiment 0: Processing file 20110103_Representative of the Turkish Republic of Northern Cyprus_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Fiename:  20110103_Representative of the Turkish Republic of Northern Cyprus_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:11:14,237 - INFO - Text length: 4652, Chunk size: 2958\n",
      "2025-02-19 11:11:14,239 - INFO - Generated chunk: CO OMB NO. 1124-0003 U.S. Department of Justice Wa... with length 4651\n",
      "2025-02-19 11:11:14,240 - INFO - Generated 1 chunks with total text length 4651\n",
      "2025-02-19 11:11:14,241 - INFO - Generated 1 chunks with total prompt token size 716\n",
      "2025-02-19 11:11:14,245 - INFO - Chunk 1/1: Token Count = 1062\n",
      "2025-02-19 11:11:14,247 - INFO - Experiment 0: Processing file 19850801_St. Lucia Tourist Board_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Fiename:  19850801_St. Lucia Tourist Board_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:11:28,256 - INFO - Text length: 2079, Chunk size: 2958\n",
      "2025-02-19 11:11:28,258 - INFO - Generated chunk: , Department of Justice Â· Washington, DC 20530 Ame... with length 2075\n",
      "2025-02-19 11:11:28,260 - INFO - Generated 1 chunks with total text length 2075\n",
      "2025-02-19 11:11:28,261 - INFO - Generated 1 chunks with total prompt token size 347\n",
      "2025-02-19 11:11:28,263 - INFO - Chunk 1/1: Token Count = 607\n",
      "2025-02-19 11:11:28,265 - INFO - Experiment 0: Processing file 19830401_Bermuda Tourism Authority_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Fiename:  19830401_Bermuda Tourism Authority_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:11:39,588 - INFO - Text length: 2150, Chunk size: 2958\n",
      "2025-02-19 11:11:39,590 - INFO - Generated chunk: OMB No. 43-R226 Approval Expires Oct. 31, 1981 UNI... with length 2148\n",
      "2025-02-19 11:11:39,591 - INFO - Generated 1 chunks with total text length 2148\n",
      "2025-02-19 11:11:39,592 - INFO - Generated 1 chunks with total prompt token size 361\n",
      "2025-02-19 11:11:39,595 - INFO - Chunk 1/1: Token Count = 617\n",
      "2025-02-19 11:11:39,597 - INFO - Experiment 0: Processing file 20180105_Hill and Knowlton Strategies, LLC_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Fiename:  20180105_Hill and Knowlton Strategies, LLC_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:11:53,184 - INFO - Text length: 5192, Chunk size: 2958\n",
      "2025-02-19 11:11:53,186 - INFO - Generated chunk: Received by NSD/FARA Registration Unit 01/05/2018 ... with length 5191\n",
      "2025-02-19 11:11:53,188 - INFO - Generated 1 chunks with total text length 5191\n",
      "2025-02-19 11:11:53,189 - INFO - Generated 1 chunks with total prompt token size 785\n",
      "2025-02-19 11:11:53,193 - INFO - Chunk 1/1: Token Count = 1166\n",
      "2025-02-19 11:11:53,194 - INFO - Experiment 0: Processing file 20170224_Alston _ Bird, LLP_Amendment_Amendment.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s---------------Fiename:  20170224_Alston _ Bird, LLP_Amendment_Amendment.pdf\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(params):\n",
    "    sample_size, use_custom_ocr, prompt_type, chunk_size_category, kwargs, experiment_id = params\n",
    "    try:\n",
    "        result = perform_experiment_token_count(sample_size, use_custom_ocr, experiment_id, prompt_type, chunk_size_category, **kwargs)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in experiment {experiment_id}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "def test_experiment_serially():\n",
    "    for i, params in enumerate(experiment_params):\n",
    "        try:\n",
    "            result = run_experiment(params)\n",
    "            if not result.empty:\n",
    "                print(\"Experiment completed successfully.\")\n",
    "            else:\n",
    "                print(\"Experiment did not return any results.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Experiment resulted in an exception: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_params = [(exp['sample_size'], exp['use_custom_ocr'], exp['prompt_type'], exp['chunk_size_category'], {\n",
    "                         'example_num': exp['example_num'], 'no_schema': exp['no_schema'], 'transformation_method': exp['transformation_method'],\n",
    "                         'chunking_method': exp['chunking_method'], 'level_type': exp['level_type']\n",
    "                         }, exp['experiment_id']) for exp in combinations]\n",
    "    \n",
    "    test_experiment_serially()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436850f4-2fb4-4b86-8d24-4ecd0c374ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
